"""
æ™ºèƒ½æ‰‹æœºè®¾è®¡ç»´åº¦åˆ†æ - å…¨åŠŸèƒ½æ•´åˆç‰ˆ
åŠŸèƒ½ï¼šä»æ•°æ®åŠ è½½åˆ°å…³é”®å› ç´ è¯†åˆ«ï¼Œæ‰€æœ‰ä»£ç é›†æˆåœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­
"""

# ================== å¯¼å…¥ä¾èµ– ==================
import re
import json
import jieba
import numpy as np
import pandas as pd
from pathlib import Path
from collections import defaultdict
from snownlp import SnowNLP

# ================== é…ç½®å‚æ•° ==================
DATA_PATH = r"D:\data\smartphone-design-pricing\data\tmall_782189145082_comments.csv"          # è¾“å…¥æ•°æ®è·¯å¾„
OUTPUT_DIR = "results"                   # è¾“å‡ºç›®å½•
STOPWORDS = {"çš„", "äº†", "æ˜¯", "åœ¨", "å’Œ"}  # åŸºç¡€åœç”¨è¯
KEYWORD_CONFIG = {
    "å¤–è§‚è®¾è®¡": ["å¤–è§‚", "é¢œè‰²", "æ‰‹æ„Ÿ", "å±å¹•", "è®¾è®¡", "é¢œå€¼"],
    "æ‹ç…§æ€§èƒ½": ["æ‹ç…§", "æ¸…æ™°", "åƒç´ ", "æ‘„åƒ", "ç›¸æœº", "å¾®è·", "æŠ“æ‹"],
    "ç»­èˆªå‘çƒ­": ["ç»­èˆª", "å‘çƒ­", "å……ç”µ", "è€—ç”µ", "ç”µæ± ", "æ¸©åº¦"],
    "ç³»ç»Ÿæ€§èƒ½": ["æµç•…", "ä¸å¡", "è¿è¡Œ", "é€Ÿåº¦", "ç³»ç»Ÿ", "å“åº”"],
    "å”®åæœåŠ¡": ["å®¢æœ", "æœåŠ¡", "ç‰©æµ", "é€€è´§", "å“åº”", "æ²Ÿé€š"],
    "ä»·æ ¼æ•æ„Ÿåº¦": ["ä»·æ ¼", "å€¼ä¸å€¼", "ä¾¿å®œ", "ä¼˜æƒ ", "åˆ’ç®—", "å¤ªè´µ", "è´µ"]
}

# ================== å·¥å…·å‡½æ•° ==================
def preprocess_text(text):
    """æ–‡æœ¬é¢„å¤„ç†ï¼ˆå†…ç½®åœç”¨è¯ï¼‰"""
    text = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9]", "", text)
    words = [word for word in jieba.lcut(text) 
            if word not in STOPWORDS and len(word) > 1]
    return words

# ================== æƒ…æ„Ÿåˆ†æ ==================
def analyze_sentiment():
    """æ‰§è¡Œæƒ…æ„Ÿåˆ†ææµç¨‹"""
    # åŠ è½½æ•°æ®
    df = pd.read_csv(DATA_PATH)
    df = df[df["content"].notna()]
    print(f"ğŸ“‚ å·²åŠ è½½ {len(df)} æ¡è¯„è®º")

    # åˆå§‹åŒ–ç»Ÿè®¡
    dim_scores = defaultdict(list)
    dim_counts = defaultdict(int)

    # å¤„ç†æ¯æ¡è¯„è®º
    for text in df["content"]:
        words = set(preprocess_text(str(text)))
        sentiment = SnowNLP(text).sentiments

        # ç»´åº¦åŒ¹é…
        for dim, keywords in KEYWORD_CONFIG.items():
            if any(kw in words for kw in keywords):
                dim_scores[dim].append(sentiment)
                dim_counts[dim] += 1

    # ç”Ÿæˆç»“æœè¡¨
    output = []
    for dim in KEYWORD_CONFIG:
        scores = dim_scores.get(dim, [])
        count = len(scores)
        avg = round(np.mean(scores), 3) if scores else 0.0
        bad_ratio = round(np.mean([s < 0.4 for s in scores]), 3) if scores else 0.0
        
        output.append({
            "è®¾è®¡ç»´åº¦": dim,
            "æåŠæ¬¡æ•°": count,
            "å¹³å‡æƒ…æ„Ÿåˆ†": avg,
            "è´Ÿé¢è¯„ä»·ç‡": bad_ratio
        })

    result_df = pd.DataFrame(output)
    return result_df

# ================== å…³é”®å› ç´ è¯†åˆ« ==================
def calculate_priority(df):
    """è®¡ç®—ç»´åº¦ä¼˜å…ˆçº§"""
    # ç†µæƒæ³•è®¡ç®—æƒé‡
    X = df[["æåŠæ¬¡æ•°", "å¹³å‡æƒ…æ„Ÿåˆ†", "è´Ÿé¢è¯„ä»·ç‡"]]
    X_norm = (X - X.min()) / (X.max() - X.min() + 1e-10)
    
    p = X_norm / (X_norm.sum(axis=0) + 1e-10)
    e = -np.sum(p * np.log(p + 1e-10), axis=0) / np.log(len(X))
    weights = (1 - e) / np.sum(1 - e)
    
    # ç»¼åˆå¾—åˆ†
    df["ä¼˜å…ˆçº§å¾—åˆ†"] = np.dot(X_norm, weights)
    df = df.sort_values("ä¼˜å…ˆçº§å¾—åˆ†", ascending=False)
    return df

# ================== ä¸»æµç¨‹ ==================
if __name__ == "__main__":
    # æ‰§è¡Œæƒ…æ„Ÿåˆ†æ
    sentiment_df = analyze_sentiment()
    
    # è®¡ç®—å…³é”®å› ç´ 
    priority_df = calculate_priority(sentiment_df)
    
    # ä¿å­˜ç»“æœ
    Path(OUTPUT_DIR).mkdir(exist_ok=True)
    priority_df.to_csv(f"{OUTPUT_DIR}/priority_results.csv", index=False)
    
    # æ‰“å°ç»“æœ
    print("\nğŸ” å…³é”®ç»´åº¦ä¼˜å…ˆçº§æ’åºï¼š")
    print(priority_df[["è®¾è®¡ç»´åº¦", "ä¼˜å…ˆçº§å¾—åˆ†"]].head(10))
    print(f"\nâœ… ç»“æœå·²ä¿å­˜è‡³ {OUTPUT_DIR}/ ç›®å½•")